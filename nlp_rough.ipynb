{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 1, 2, 3],\n",
       "        [1, 2, 3, 4],\n",
       "        [2, 3, 4, 5]],\n",
       "\n",
       "       [[0, 1, 2, 3],\n",
       "        [1, 2, 3, 4],\n",
       "        [2, 3, 4, 5]],\n",
       "\n",
       "       [[0, 1, 2, 3],\n",
       "        [1, 2, 3, 4],\n",
       "        [2, 3, 4, 5]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5]], [[0, 1, 2, 3], [1, 2, 3 ,4 ], [2, 3, 4, 5]], [[0, 1, 2, 3], [1, 2, 3 ,4 ], [2, 3, 4, 5]]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 4)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 4)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 4)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(x, axis=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk                                  # Python library for NLP\n",
    "from nltk.corpus import twitter_samples      # sample Twitter dataset from NLTK\n",
    "import matplotlib.pyplot as plt              # visualization library\n",
    "import numpy as np \n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    \"\"\"Process tweet function.\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks    \n",
    "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):  # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/obaji/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/obaji/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# download the stopwords for the process_tweet function\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# import our convenience functions\n",
    "from utils import process_tweet, build_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets:  10000\n"
     ]
    }
   ],
   "source": [
    "# select the lists of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "# concatenate the lists, 1st part is the positive tweets followed by the negative\n",
    "tweets = all_positive_tweets + all_negative_tweets\n",
    "\n",
    "# let's see how many tweets we have\n",
    "print(\"Number of tweets: \", len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.append(np.ones((len(all_positive_tweets))), np.zeros((len(all_negative_tweets))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('followfriday', 1.0) 25\n",
      "('top', 1.0) 32\n",
      "('engag', 1.0) 7\n",
      "('member', 1.0) 16\n",
      "('commun', 1.0) 33\n",
      "('week', 1.0) 83\n",
      "(':)', 1.0) 3691\n",
      "('hey', 1.0) 77\n",
      "('jame', 1.0) 7\n",
      "('odd', 1.0) 2\n",
      "(':/', 1.0) 5\n",
      "('pleas', 1.0) 99\n",
      "('call', 1.0) 37\n",
      "('contact', 1.0) 7\n",
      "('centr', 1.0) 2\n",
      "('02392441234', 1.0) 1\n",
      "('abl', 1.0) 8\n",
      "('assist', 1.0) 1\n",
      "('mani', 1.0) 33\n",
      "('thank', 1.0) 643\n",
      "('listen', 1.0) 17\n",
      "('last', 1.0) 47\n",
      "('night', 1.0) 68\n",
      "('bleed', 1.0) 2\n",
      "('amaz', 1.0) 51\n",
      "('track', 1.0) 5\n",
      "('scotland', 1.0) 2\n",
      "('congrat', 1.0) 21\n",
      "('yeaaah', 1.0) 1\n",
      "('yipppi', 1.0) 1\n",
      "('accnt', 1.0) 2\n",
      "('verifi', 1.0) 2\n",
      "('rqst', 1.0) 1\n",
      "('succeed', 1.0) 1\n",
      "('got', 1.0) 69\n",
      "('blue', 1.0) 9\n",
      "('tick', 1.0) 1\n",
      "('mark', 1.0) 1\n",
      "('fb', 1.0) 6\n",
      "('profil', 1.0) 2\n",
      "('15', 1.0) 5\n",
      "('day', 1.0) 246\n",
      "('one', 1.0) 131\n",
      "('irresist', 1.0) 2\n",
      "('flipkartfashionfriday', 1.0) 17\n",
      "('like', 1.0) 233\n",
      "('keep', 1.0) 68\n",
      "('love', 1.0) 401\n",
      "('custom', 1.0) 4\n",
      "('wait', 1.0) 70\n",
      "('long', 1.0) 36\n",
      "('hope', 1.0) 143\n",
      "('enjoy', 1.0) 79\n",
      "('happi', 1.0) 212\n",
      "('friday', 1.0) 116\n",
      "('lwwf', 1.0) 1\n",
      "('second', 1.0) 10\n",
      "('thought', 1.0) 29\n",
      "('’', 1.0) 21\n",
      "('enough', 1.0) 18\n",
      "('time', 1.0) 128\n",
      "('dd', 1.0) 1\n",
      "('new', 1.0) 146\n",
      "('short', 1.0) 7\n",
      "('enter', 1.0) 9\n",
      "('system', 1.0) 2\n",
      "('sheep', 1.0) 1\n",
      "('must', 1.0) 19\n",
      "('buy', 1.0) 12\n",
      "('jgh', 1.0) 4\n",
      "('go', 1.0) 151\n",
      "('bayan', 1.0) 1\n",
      "(':d', 1.0) 658\n",
      "('bye', 1.0) 8\n",
      "('act', 1.0) 8\n",
      "('mischiev', 1.0) 1\n",
      "('etl', 1.0) 1\n",
      "('layer', 1.0) 1\n",
      "('in-hous', 1.0) 1\n",
      "('wareh', 1.0) 1\n",
      "('app', 1.0) 16\n",
      "('katamari', 1.0) 1\n",
      "('well', 1.0) 81\n",
      "('…', 1.0) 38\n",
      "('name', 1.0) 18\n",
      "('impli', 1.0) 1\n",
      "(':p', 1.0) 139\n",
      "('influenc', 1.0) 18\n",
      "('big', 1.0) 34\n",
      "('...', 1.0) 290\n",
      "('juici', 1.0) 3\n",
      "('selfi', 1.0) 12\n",
      "('follow', 1.0) 447\n",
      "('u', 1.0) 245\n",
      "('back', 1.0) 163\n",
      "('perfect', 1.0) 24\n",
      "('alreadi', 1.0) 28\n",
      "('know', 1.0) 155\n",
      "(\"what'\", 1.0) 17\n",
      "('great', 1.0) 172\n",
      "('opportun', 1.0) 23\n",
      "('junior', 1.0) 2\n",
      "('triathlet', 1.0) 1\n",
      "('age', 1.0) 2\n",
      "('12', 1.0) 5\n",
      "('13', 1.0) 6\n",
      "('gatorad', 1.0) 1\n",
      "('seri', 1.0) 5\n",
      "('get', 1.0) 209\n",
      "('entri', 1.0) 4\n",
      "('lay', 1.0) 4\n",
      "('greet', 1.0) 5\n",
      "('card', 1.0) 8\n",
      "('rang', 1.0) 3\n",
      "('print', 1.0) 4\n",
      "('today', 1.0) 113\n",
      "('job', 1.0) 41\n",
      "(':-)', 1.0) 701\n",
      "(\"friend'\", 1.0) 3\n",
      "('lunch', 1.0) 5\n",
      "('yummm', 1.0) 1\n",
      "('nostalgia', 1.0) 1\n",
      "('tb', 1.0) 2\n",
      "('ku', 1.0) 1\n",
      "('id', 1.0) 8\n",
      "('conflict', 1.0) 1\n",
      "('help', 1.0) 44\n",
      "(\"here'\", 1.0) 25\n",
      "('screenshot', 1.0) 3\n",
      "('work', 1.0) 111\n",
      "('hi', 1.0) 173\n",
      "('liv', 1.0) 2\n",
      "('hello', 1.0) 59\n",
      "('need', 1.0) 78\n",
      "('someth', 1.0) 28\n",
      "('fm', 1.0) 2\n",
      "('twitter', 1.0) 29\n",
      "('—', 1.0) 27\n",
      "('sure', 1.0) 59\n",
      "('thing', 1.0) 69\n",
      "('dm', 1.0) 39\n",
      "('x', 1.0) 72\n",
      "(\"i'v\", 1.0) 35\n",
      "('heard', 1.0) 9\n",
      "('four', 1.0) 5\n",
      "('season', 1.0) 9\n",
      "('pretti', 1.0) 20\n",
      "('dope', 1.0) 2\n",
      "('penthous', 1.0) 1\n",
      "('obv', 1.0) 1\n",
      "('gobigorgohom', 1.0) 1\n",
      "('fun', 1.0) 58\n",
      "(\"y'all\", 1.0) 4\n",
      "('yeah', 1.0) 47\n",
      "('suppos', 1.0) 7\n",
      "('lol', 1.0) 64\n",
      "('chat', 1.0) 13\n",
      "('bit', 1.0) 20\n",
      "('youth', 1.0) 19\n",
      "('💅🏽', 1.0) 1\n",
      "('💋', 1.0) 2\n",
      "('seen', 1.0) 10\n",
      "('year', 1.0) 43\n",
      "('rest', 1.0) 12\n",
      "('goe', 1.0) 7\n",
      "('quickli', 1.0) 3\n",
      "('bed', 1.0) 16\n",
      "('music', 1.0) 21\n",
      "('fix', 1.0) 10\n",
      "('dream', 1.0) 20\n",
      "('spiritu', 1.0) 1\n",
      "('ritual', 1.0) 1\n",
      "('festiv', 1.0) 8\n",
      "('népal', 1.0) 1\n",
      "('begin', 1.0) 4\n",
      "('line-up', 1.0) 4\n",
      "('left', 1.0) 13\n",
      "('see', 1.0) 186\n",
      "('sarah', 1.0) 4\n",
      "('send', 1.0) 23\n",
      "('us', 1.0) 115\n",
      "('email', 1.0) 26\n",
      "('bitsy@bitdefender.com', 1.0) 1\n",
      "(\"we'll\", 1.0) 20\n",
      "('asap', 1.0) 5\n",
      "('kik', 1.0) 22\n",
      "('hatessuc', 1.0) 1\n",
      "('32429', 1.0) 1\n",
      "('kikm', 1.0) 1\n",
      "('lgbt', 1.0) 2\n",
      "('tinder', 1.0) 1\n",
      "('nsfw', 1.0) 1\n",
      "('akua', 1.0) 1\n",
      "('cumshot', 1.0) 1\n",
      "('come', 1.0) 71\n",
      "('hous', 1.0) 7\n",
      "('nsn_supplement', 1.0) 1\n",
      "('effect', 1.0) 4\n",
      "('press', 1.0) 1\n",
      "('releas', 1.0) 11\n",
      "('distribut', 1.0) 1\n",
      "('result', 1.0) 2\n",
      "('link', 1.0) 19\n",
      "('remov', 1.0) 3\n",
      "('pressreleas', 1.0) 1\n",
      "('newsdistribut', 1.0) 1\n",
      "('bam', 1.0) 44\n",
      "('bestfriend', 1.0) 50\n",
      "('lot', 1.0) 87\n",
      "('warsaw', 1.0) 44\n",
      "('<3', 1.0) 135\n",
      "('x46', 1.0) 1\n",
      "('everyon', 1.0) 58\n",
      "('watch', 1.0) 47\n",
      "('documentari', 1.0) 1\n",
      "('earthl', 1.0) 2\n",
      "('youtub', 1.0) 13\n",
      "('support', 1.0) 27\n",
      "('buuut', 1.0) 1\n",
      "('oh', 1.0) 53\n",
      "('look', 1.0) 139\n",
      "('forward', 1.0) 29\n",
      "('visit', 1.0) 30\n"
     ]
    }
   ],
   "source": [
    "yslist = np.squeeze(labels).tolist()\n",
    "\n",
    "# Start with an empty dictionary and populate it by looping over all tweets\n",
    "# and over all processed words in each tweet.\n",
    "freqs = {}\n",
    "for y, tweet in zip(yslist, tweets):\n",
    "    for word in process_tweet(tweet):\n",
    "        pair = (word, y)\n",
    "        if pair in freqs:\n",
    "            freqs[pair] += 1\n",
    "        else:\n",
    "            freqs[pair] = 1\n",
    "for key, value in freqs.items():\n",
    "    print(key, value)\n",
    "    if freqs[key] == 30:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.append(np.ones((4, 1)), np.zeros((4, 1)), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "64f7cc407693abb8d0542334aa4275d1cfd68b304f46c77cd6302912cf6e87bf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
